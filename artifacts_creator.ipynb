{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963586f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "ARTICLES_PATH = Path(\"news-portal-user-interactions-by-globocom/articles_metadata.csv\")\n",
    "EMBEDDINGS_PATH = Path(\"news-portal-user-interactions-by-globocom/articles_embeddings.pickle\")\n",
    "CLICKS_DIR = Path(\"news-portal-user-interactions-by-globocom/clicks\")\n",
    "OUT_DIR = Path(\"artifacts\")\n",
    "\n",
    "HALF_LIFE_DAYS = 30\n",
    "ALPHA_RECENCY = 0.6\n",
    "TOPK_COLD = 5\n",
    "\n",
    "MIN_WORDS = 0 \n",
    "\n",
    "ENABLE_PCA = True\n",
    "PCA_VARIANCE = 0.95\n",
    "PCA_N_COMPONENTS = 52\n",
    "PCA_DTYPE = \"float32\"\n",
    "\n",
    "NORMALIZE_L2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e2ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(mat: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(mat, axis=1, keepdims=True)\n",
    "    n = np.maximum(n, eps)\n",
    "    return (mat / n).astype(mat.dtype, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d9d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles_and_embeddings(articles_path=ARTICLES_PATH, embeddings_path=EMBEDDINGS_PATH, min_words=MIN_WORDS):\n",
    "    df = pd.read_csv(articles_path)\n",
    "\n",
    "    # masque pour éliminer les articles vides\n",
    "    mask = df['words_count'] > min_words\n",
    "\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        E = pickle.load(f)  # np.ndarray (N, 250)\n",
    "\n",
    "    df = df[mask].copy()\n",
    "    E = E[mask.values]\n",
    "\n",
    "    df['pub_ts'] = (df['created_at_ts'] // 1000).astype('int64')\n",
    "\n",
    "    df = df[['article_id', 'pub_ts', 'words_count']].reset_index(drop=True)\n",
    "    df[\"article_id\"] = df[\"article_id\"].astype(str)\n",
    "\n",
    "    E = E.astype('float32' if PCA_DTYPE == \"float32\" else 'float64', copy=False)\n",
    "    return df, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c561a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clicks(clicks_dir=CLICKS_DIR):\n",
    "    files = sorted(Path(clicks_dir).glob(\"*.csv\"))\n",
    "    usecols = [\"user_id\", \"click_article_id\", \"click_timestamp\"]\n",
    "    parts = [pd.read_csv(p, usecols=usecols) for p in files]\n",
    "    df = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    df.rename(columns={\"click_article_id\": \"article_id\"}, inplace=True)\n",
    "    df[\"timestamp\"] = (df[\"click_timestamp\"] // 1000).astype(\"int64\")\n",
    "    df = df[[\"user_id\", \"article_id\", \"timestamp\"]]\n",
    "\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"article_id\"] = df[\"article_id\"].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288b8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_apply_pca(E: np.ndarray):\n",
    "    n_components = PCA_N_COMPONENTS if PCA_N_COMPONENTS is not None else float(PCA_VARIANCE)\n",
    "    pca = PCA(n_components=n_components, svd_solver=\"full\", random_state=0)\n",
    "    X_pca = pca.fit_transform(E)\n",
    "    exp_var = pca.explained_variance_ratio_.astype('float32')\n",
    "    cum_exp = np.cumsum(exp_var)\n",
    "    report = {\n",
    "        \"K\": int(X_pca.shape[1]),\n",
    "        \"variance_expliquee_cumulee\": float(cum_exp[-1]),\n",
    "        \"variance_par_composante\": exp_var.tolist()\n",
    "    }\n",
    "    return X_pca, pca, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "814ecf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1) Articles + embeddings (filtrés)\n",
    "    articles, E = load_articles_and_embeddings()\n",
    "    id_to_row = {aid: i for i, aid in enumerate(articles[\"article_id\"].tolist())}\n",
    "\n",
    "    # 2) ACP (optionnel)\n",
    "    if ENABLE_PCA:\n",
    "        X_pca, pca, pca_report = fit_and_apply_pca(E)\n",
    "        if NORMALIZE_L2:\n",
    "            X_pca = l2_normalize(X_pca)\n",
    "        E_eff = X_pca\n",
    "        K_eff = int(E_eff.shape[1])\n",
    "\n",
    "        # Sauvegardes PCA\n",
    "        with open(OUT_DIR / \"pca_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pca, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        (OUT_DIR / \"pca_report.json\").write_text(json.dumps(pca_report, indent=2), encoding=\"utf-8\")\n",
    "    else:\n",
    "        # Option sans PCA (éventuellement normalisation L2)\n",
    "        E_eff = l2_normalize(E) if NORMALIZE_L2 else E\n",
    "        K_eff = int(E_eff.shape[1])\n",
    "\n",
    "    # 3) Clicks (restreints aux articles conservés)\n",
    "    clicks = load_clicks(CLICKS_DIR)\n",
    "    clicks = clicks[clicks[\"article_id\"].isin(articles[\"article_id\"])]\n",
    "\n",
    "    # --- Référentiel temporel ---\n",
    "    t_min, t_max = int(clicks[\"timestamp\"].min()), int(clicks[\"timestamp\"].max())\n",
    "    ref_now = t_max\n",
    "    data_span = max(1, t_max - t_min)\n",
    "\n",
    "    # fenêtre 30 jours bornée par l’empan réel\n",
    "    thirty_days = 30 * 24 * 3600\n",
    "    win_sec = min(thirty_days, data_span)\n",
    "    cutoff = ref_now - win_sec\n",
    "\n",
    "    # 3a) Popularité\n",
    "    pop_all = clicks.groupby(\"article_id\").size().rename(\"pop_all\")\n",
    "    pop_30 = clicks[clicks[\"timestamp\"] >= cutoff].groupby(\"article_id\").size().rename(\"pop_30d\")\n",
    "    art = articles.merge(pop_all, left_on=\"article_id\", right_index=True, how=\"left\")\n",
    "    art = art.merge(pop_30, left_on=\"article_id\", right_index=True, how=\"left\")\n",
    "    art[[\"pop_all\", \"pop_30d\"]] = art[[\"pop_all\", \"pop_30d\"]].fillna(0).astype(\"int64\")\n",
    "\n",
    "    # 3b) Profils utilisateurs (dans l’espace réduit E_eff)\n",
    "    user_ids, profiles = [], []\n",
    "    hl_sec = HALF_LIFE_DAYS * 24 * 3600.0\n",
    "    lam = math.log(2) / hl_sec if hl_sec > 0 else 0.0\n",
    "    idx_df = pd.DataFrame({\"article_id\": art[\"article_id\"], \"row\": range(len(art))})\n",
    "    c2 = clicks.merge(idx_df, on=\"article_id\", how=\"inner\")\n",
    "    for uid, grp in c2.groupby(\"user_id\"):\n",
    "        rows = grp[\"row\"].to_numpy()\n",
    "        age = (ref_now - grp[\"timestamp\"].to_numpy()).astype(\"float64\")\n",
    "        w = np.exp(-lam * age) if lam > 0 else np.ones_like(age)\n",
    "        # moyenne pondérée dans l’espace réduit\n",
    "        v = (E_eff[rows].astype(\"float64\") * w[:, None]).sum(axis=0) / (w.sum() + 1e-12)\n",
    "        profiles.append(v.astype(\"float32\"))\n",
    "        user_ids.append(uid)\n",
    "    P = np.vstack(profiles).astype(\"float32\")\n",
    "    if NORMALIZE_L2:\n",
    "        P = l2_normalize(P)\n",
    "\n",
    "    # 3c) Cold-start\n",
    "    rec = (art[\"pub_ts\"] - art[\"pub_ts\"].min()) / max(1, (art[\"pub_ts\"].max() - art[\"pub_ts\"].min()))\n",
    "    pop_base = art[\"pop_30d\"] if art[\"pop_30d\"].max() > 0 else art[\"pop_all\"]\n",
    "    pop = (pop_base - pop_base.min()) / max(1, (pop_base.max() - pop_base.min()))\n",
    "    score = ALPHA_RECENCY * rec + (1 - ALPHA_RECENCY) * pop\n",
    "    top = art.assign(score=score).sort_values([\"score\", \"pub_ts\", \"pop_30d\", \"pop_all\"],\n",
    "                                              ascending=[False, False, False, False])\n",
    "    cold5 = top.head(TOPK_COLD)[\"article_id\"].tolist()\n",
    "    top200 = top.head(200)\n",
    "\n",
    "    # 4) Sauvegarde artefacts\n",
    "    art.to_parquet(OUT_DIR / \"articles_clean.parquet\", index=False)\n",
    "\n",
    "    # Embeddings articles (réduits et/ou normalisés)\n",
    "    np.save(OUT_DIR / \"embeddings_clean.npy\", E_eff.astype(\"float32\"))\n",
    "\n",
    "    # Indexations & profils\n",
    "    (OUT_DIR / \"id_to_row.json\").write_text(json.dumps(id_to_row))\n",
    "    np.save(OUT_DIR / \"user_profiles.npy\", P)\n",
    "    pd.Series(user_ids).to_csv(OUT_DIR / \"user_ids.csv\", index=False, header=False)\n",
    "    (OUT_DIR / \"user_to_idx.json\").write_text(json.dumps({u: i for i, u in enumerate(user_ids)}))\n",
    "\n",
    "    # Cold-start et popularité\n",
    "    (OUT_DIR / \"cold_start_top5.json\").write_text(json.dumps(cold5))\n",
    "    top200.to_parquet(OUT_DIR / \"popular_recent.parquet\", index=False)\n",
    "\n",
    "    # Manifest\n",
    "    manifest = {\n",
    "        \"built_at\": int(datetime.now(timezone.utc).timestamp()),\n",
    "        \"articles\": int(len(art)),\n",
    "        \"embedding_dim\": K_eff,\n",
    "        \"users\": int(len(user_ids)),\n",
    "        \"half_life_days\": HALF_LIFE_DAYS,\n",
    "        \"alpha_recency\": ALPHA_RECENCY,\n",
    "        \"topk_cold\": TOPK_COLD,\n",
    "        \"pca_enabled\": ENABLE_PCA,\n",
    "        \"pca_variance_target\": PCA_VARIANCE if ENABLE_PCA and PCA_N_COMPONENTS is None else None,\n",
    "        \"pca_n_components\": PCA_N_COMPONENTS if ENABLE_PCA and PCA_N_COMPONENTS is not None else K_eff,\n",
    "        \"normalized_l2\": NORMALIZE_L2\n",
    "    }\n",
    "    (OUT_DIR / \"build_manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "    print(json.dumps(manifest, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "575989ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"built_at\": 1759010297,\n",
      "  \"articles\": 364012,\n",
      "  \"embedding_dim\": 52,\n",
      "  \"users\": 322897,\n",
      "  \"half_life_days\": 30,\n",
      "  \"alpha_recency\": 0.6,\n",
      "  \"topk_cold\": 5,\n",
      "  \"pca_enabled\": true,\n",
      "  \"pca_variance_target\": null,\n",
      "  \"pca_n_components\": 52,\n",
      "  \"normalized_l2\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
